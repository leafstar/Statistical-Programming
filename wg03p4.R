## Group 3  Muxing Wang (s2201749); Nutsa Tokhadze (s1778736); Karla Itzel Vega Ortega (s2126801) 
## Address of github repo: https://github.com/leafstar/Statistical-Programming


                                              ## A BFGS optimizer ##
                                                   ## Overview ##

## The aim of the project is to implement optimization with BFGS quasi-Newton minimization method. Maximum likelihood 
## optimization, and many other statistical learning approaches, rely on numerical optimization. Optimization is finding a 
## maximum or minimum of a function. Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is one of the optimization methods 
## that approximate the second derivative (Hessian) where the second derivative cannot be calculated. It updates the inverse
## approximate Hessian, thereby avoiding the need for a matrix solve. The BFGS algorithm is perhaps one of the most widely
## used second-order algorithms for numerical optimization.

                                              ## Outline of the Functions ##

## The Task focus on writing an R function, bfgs, implementing the BFGS quasi-Newton minimization method. Bfgs function returns:
## (1) f -the scalar value of the objective function at the minimum, 
## (2) theta -the vector of values of the parameters at the minimum, 
## (3) iter- the number of iterations taken to reach the minimum, 
## (4) g - the gradient vector at the minimum and, 
## (5) H - the approximate Hessian matrix.
## Additionally, function issues errors or warnings in at least the following cases. 
## (1) If the objective or derivatives are not finite at the initial theta; 
## (2) If the step fails to reduce the objective but convergence has not occurred 
## (3) If maxit is reached without convergence and,
## (4) If the supplied f does not supply a gradient attribute when requested- in this case the gradient by finite differencing
## is computed. 
## There are other considerations as well: 
## (1) if a step leads to a larger objective value the step length is reduced; 
##     if a step size cannot satisfy the second Wolfe condition, it is inncreased.
## Anyway, Step length should reduces the objective function and satisfies the second Wolfe condition (use c2 = 0.9). 
## (2) To judge whether the gradient vector is close enough to zero, the magnitude of the objective is reconsidered 
## (3) The final approximate Hessian matrix is generated by finite differencing the gradient vector. 
## (4) When p is the number of parameters, then the update of the approximate inverse Hessian has cost O(p^2) not O(p^3).

## For above mentioned tasks following functions were created:
## (1) get_grad- computes gradient by finite differencing, when f does not supply a gradient attribute when requested.
## (2) get_step_size- searches for the right step length that reduces the objective function and satisfies the second Wolfe 
## condition.  
## (3) bfgs - 
## Which returns: 
##       (1) f -the scalar value of the objective function at the minimum, 
##       (2) theta -the vector of values of the parameters at the minimum, 
##       (3) iter- the number of iterations taken to reach the minimum, 
##       (4) g - the gradient vector at the minimum and 
##       (5) H - the approximate Hessian matrix.



                                      ## 1.Computing gradient by finite differencing ##


## get_grad function computes gradient by finite differencing, when f does not supply a gradient attribute when requested.
## It is a function of supplied f- objective function; theta- the vector of values of the parameters;
## and eps-epsilon, the finite differencing interval.

get_grad <- function(f,...,theta,eps){                      ## defines get_grad function 
  if(is.null(attributes(f(theta, TRUE,...))$gradient)){     ## if f does not supply a gradient attribute computes gradient by user 
    ## finite differencing
    grad = theta                                            ## initializes gradient to theta 
    for (i in 1:length(theta)) {                            ## calculates for loops over parameters
      theta1 <- theta; theta1[i] <- theta1[i] + eps         ## increases theta by a small distance of eps
      grad[i] <- (f(theta1,...) - f(theta,...))/eps         ## numerically calculates the slope
    }
    as.vector(grad)                                         ## gets gradient as a vector
  }else{                                                    ## in other case - if f supply a gradient attribute
    attributes(f(theta, TRUE,...))$gradient                 ## gets gradient attribute from f
  }
}

                                    ## 2. Searching for the right steps ##

## get_step_size function searches for the right step length that reduces the objective function and satisfies the second Wolfe 
## condition. It is function of f- objective function; theta- the vector of values of the parameters and 
## d-direction.
## we will use binary search to get the valid step size which satisfies the aforementioned conditions.

get_step_size <- function(f,...,theta,d){                    ## defines get_step_size function
  max.iter = 20                                              ## since binary search is O(logn), this bound is big enough for us to find a small step size.
  iter = 0                                                   ## Initializes iterations
  eps = 1e-7                                                 ## epsilon used to do finite differencing to get the gradient
  c1 = 1e-4                                                  ## first wolfe condition parameter
  c2 = 0.9                                                   ## second wolfe condition parameter
  u = 1                                                      ## upper bound for step size
  l = 0                                                      ## lower bound for step size
  alpha = 1                                                  ## step size is set to 1 initially
  theta.prime = theta + alpha*d                              ## Set the values of theta.prime based on parameters and direction
  g = get_grad(f = f,...,theta = theta,eps = eps)            ## creates the gradient
  gamma = c1*sum(g*d)                                        ## gamma is the minimizing progress we wish to make
  
  while(iter <= max.iter){                                   ## iterates until the number max of iterations is reached
    iter = iter + 1                                          ## increment iter 
    theta.prime = theta + alpha*d                            ## calculates theta.prime, which is the next theta
    g.prime = get_grad(f = f,..., theta = theta.prime, eps = eps)  ## calculates gradient at next theta(theta.prime) 
    
    
    ## binary search, alpha is always the mean of upper and lower bounds.
    
    ## if objective value does not decrease, we reduce the step size
    if(f(theta.prime,...) >= f(theta,...) + alpha* gamma){   
      u = alpha                                              
      alpha = (l+u)/2
    }
    
    ## if Wolfe condition2 is not satisfied, we increase the step size
    else if(g.prime %*% d /(g %*% d) > c2){
      l = alpha
      alpha = (l+u)/2
    }
    
    ## if both of the two conditions are satisfied, we break out of the loop 
    else{
      break
    }
  }
  
  ## if the maximum iterations have reached, and objective value does not increase
  ## it means we fail to find a valid step size. We raised a warning to notify such info
  if(f(theta.prime,...) >= f(theta,...) + alpha* gamma){
    warning("we cannot find a valid step size")
  }
  alpha       ## returns the step size we found.
}

                                                      ## 3. BFGS ##

## bfgs function returns a (named) list containing: 
## (1) f -the scalar value of the objective function at the minimum, 
## (2) theta -the vector of values of the parameters at the minimum, 
## (3) iter- the number of iterations taken to reach the minimum, 
## (4) g - the gradient vector at the minimum and,
## (5) H - the approximate Hessian matrix. 
## bfgs is a function of
##     1.theta - a vector of initial values for the optimization parameters
##     2.f- is the objective function to minimize.  Its first argument is the vector of optimization parameters. Its second
##       argument is a logical indicating whether or not gradients of the objective w.r.t. the parameters should be computed
## NOTE Remaining arguments are passed from bfgs using ‘...’.
## tol is the convergence tolerance
## fscale is a rough estimate of the magnitude of f at the optimum - used in convergence testing
## maxit is the maximum number of BFGS iterations to try before giving up


bfgs <- function(theta,f,...,tol=1e-5,fscale=1,maxit=100){   ## defines the function "bfgs" with 5 inputs
  theta.k = theta                                ## sets theta.k to the initial values for the optimization parameters theta 
  B.k <- diag(length(theta))                     ## initializes the inverse of approximation of hessian to be the identity matrix
  f0 = f(theta.k,...)                            ## calculates f0 as the first objective value
  eps = 1e-7  ## finite difference interval
  g.k = get_grad(f=f , ...,theta = theta.k,eps =  eps) ## gets the gradient at theta
  
  
  ## If the objective or derivatives are not finite at the initial theta, we throw errors and stop the program.
  if(any(is.infinite(g.k))) stop("gradient is not finite")  
  if(is.infinite(f0)) stop("objective value is not finite")
  
  iter = 0  ## initializes iter, which is a counter of number of iterations.
  
  ## in this loop, we will keep updating theta and the inverse of approximate Hessian matrix
  ## until max iteration number has been reached or the algorithm converges.
  ## outline:
  ##  1. calculates current objective and gradient.
  ##  2. calculates the direction d as the negative of the product of inverse hessian and gradient.
  ##  3. finds a valid step size at current theta.
  ##  4. gets new theta.kprime based on the step size. 
  ##  5. calculates the gradient g.kprime at theat.kprime
  ##  6. calculates s.k, y.k as the difference between theta's and gradients,
  ##     which are (theta.kprime - theta.k), and (g.kprime - g.k) respectively.
  ##  7. calculates rho.k by taking the dot product
  ##  8. updates the inverse of approximation of hessian based on s.k, y.k, and rho.k
  ##  9. updates theta.k, B.k and increment iter.
  while(max(abs(g.k)) >= (abs(f0)+fscale)*tol && iter <= maxit){  
    
    f0 = f(theta.k,...)  ## calculates current objective value
    
    g.k = get_grad(f = f,..., theta = theta.k, eps =  eps) ## re calculates the gradient
    
    d = -B.k %*% g.k  ## calculates the direction as the negative of the product of inverse hessian and gradient.
    
    ## we called the function created in part 2 where the return is the alpha value
    alpha = get_step_size(f = f,... ,theta = theta.k, d = d) 
    
    ## create theta prime with the values of theta, alpha and direction
    theta.kprime = as.vector(theta.k + alpha * d)
    
    ## gets the gradient kprime from the get_grad function
    g.kprime = get_grad(f = f,..., theta = theta.kprime, eps = eps)
    
    ## calculates s.k as the difference of theta.kprime and theta.k
    s.k = theta.kprime - theta.k 
    
    ## calculates y.k as the difference between the gradients.
    y.k = g.kprime - g.k
    
    ## calculates rho.k inverse as the dot product of s.k and y.k
    rho.k.i = sum(s.k*y.k)
    
    ## calculates rho.k
    rho.k = 1/rho.k.i
    
    ## fast implementation
    ## updates inverse approximation of the hessian matrix in O(p^2)
    B.kprime = B.k + ((rho.k.i+ as.vector(t(y.k)%*%B.k%*%y.k))*s.k %*% t(s.k))/(rho.k.i**2) - (B.k%*%y.k%*%t(s.k)+s.k%*%(t(y.k)%*%B.k))/rho.k.i
    
    ## updates theta.k and B.k
    theta.k = theta.kprime
    B.k = B.kprime
    
    ## increment iter
    iter = iter + 1
  }
  
  ## if maximum of iterations has been reached without convergence, raises a warning.
  if(iter > maxit) warning("max iteration has been reached")
  
  ## fast way to compute H as B^-1
  H = chol2inv(chol(B.k))
  
  ## if the Hessian is not symmetric then transform it, making it symmetric.
  if (!isSymmetric(H)){
    H <- 0.5 * (t(H) + H)
  }
  
  # returns list of function, theta value, number of iterations, gradiente vector and Hessian
  list(f=f(theta.k,...), theta=theta.k, iter=iter, g=g.k, H=H)
}
