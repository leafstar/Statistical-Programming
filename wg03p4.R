## Group 3  Muxing Wang (s2201749); Nutsa Tokhadze (s1778736); Karla Itzel Vega Ortega (s2126801) 
## Address of github repo: https://github.com/leafstar/Statistical-Programming


                                                         ## A BFGS optimizer ##
                                                             # Overview #

## The aim of the project is to implement optimization with BFGS quasi-Newton minimization method. Maximum likelihood 
## optimization, and many other statistical learning approaches, rely on numerical optimization. Optimization is finding a 
## maximum or minimum of a function. Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is one of the optimiazation methods 
## that approximate the second derivative (Hessian) where the second derivative cannot be calculated. It updates the inverse
## approximate Hessian, thereby avoiding the need for a matrix solve. The BFGS algorithm is perhaps one of the most widely
## used second-order algorithms for numerical optimization.

                                                      ## Outline of the Functions ##

## The Task is to write an R function, bfgs, implementing the BFGS quasi-Newton minimization method. Bfgs function returns:
## (1) f -the scalar value of the objective function at the minimum, (2) theta -the vector of values of the parameters 
## at the minimum, (3) iter- the number of iterations taken to reach the minimum, (4) g - the gradient vector at the minimum
## and (5) H - the approximate Hessian matrix.
## Additionally, function issues errors or warnings in at least the following cases. (1) If the objective or derivatives 
## are not finite at the initial theta; (2) If the step fails to reduce the objective but convergence has not occurred (3) If 
## maxit is reached without convergence and (4) If the supplied f does not supply a gradient attribute when requested- in this 
## case the gradient by finite differencing is computed. 
## There are other considerations as well: (1) if a step leads to a non-finite objective value the step length is  reduced.
## Step length reduces the objective function and satisfie the second Wolfe condition (use c2 = 0.9). (2) To judge whether 
## the gradient vector is close enough to zero, the magnitude of the objective is reconsidered (3) The final approximate 
## Hessian matrix is generated by finite differencing the gradient vector. (4) When p is the number of parameters, then the 
## update of the approximate inverse Hessian has cost O(p^2) not O(p^3). 
## For implementing these steps the following functions were created:
## (1) get_grad- computes gradient by finite differencing, when f does not supply a gradient attribute when requested.
## (2) get_step_size- searches for the right step length that reduces the objective function and satisfies the second Wolfe 
## condition.  
## (3) bfgs - returns (1) f -the scalar value of the objective function at the minimum, (2) theta -the vector of values of the 
## parameters at the minimum, (3) iter- the number of iterations taken to reach the minimum, (4) g - the gradient vector at the
## minimum and (5) H - the approximate Hessian matrix.



                                                ## 1.Computing gradient by finite differencing ##


## get_grad function computes gradient by finite differencing, when f does not supply a gradient attribute when requested.
## It is a function of supplied f- scalar value of the objective function; theta- the vector of values of the parameters;
## and eps-epsilon
get_grad <- function(f,...,theta,eps){    # defines get_grad function 
  if(is.null(attributes(f(theta, TRUE,...))$gradient)){ # if f does not supply a gradient attribute computes gradient by
    ## finite differencing
    grad = theta                          ## initializes gradient to theta 
    for (i in 1:length(theta)) {          ## calculates for loops over parameters
      theta1 <- theta; theta1[i] <- theta1[i] + eps ## increases th0[i] by eps nll1 <- nll(th1,t=t80,y=y) ## compute resulting nll
      grad[i] <- (f(theta1,...) - f(theta,...))/eps ## approximates -dl/dth[i]
    }
    as.vector(grad)                       ## gets gradient as a vector
  }else{                                  ## in other case - if f supply a gradient attribute
    g.k = attributes(f(theta, TRUE,...))$gradient ## gets gradient attribute from f
  }
}

                                                  ## 2. Searching for the right steps ##

## get_step_size function searches for the right step length that reduces the objective function and satisfies the second Wolfe 
## condition. It is function of f- scalar value of the objective function; theta- the vector of values of the parameters and 
## d-direction. 
get_step_size <- function(f,...,theta,d){ ## defines get_step_size function
  max.iter = 40 # since we are doing a exponential decay, this bound is big enough for us to find a small step size.
  iter = 0                   #start with 0 iteration
  eps = 1e-7
  c1 = 0.1                    ## first Wolf Condition parametre
  c2 = 0.9                    ## second wolf condition parameter
  alpha = 1
  theta.prime = theta + alpha*d
  g = get_grad(f = f,...,theta = theta,eps = eps)
  gamma = c1*sum(g*d)         # for cond1 just as the video said
  cat("alphagamma",gamma,"\n")
  while(iter <= max.iter){
    iter = iter + 1
    g.prime = get_grad(f = f,..., theta = theta.prime, eps = eps)
    if(f(theta.prime,...) < f(theta,...) + alpha * gamma && g.prime %*% d /(g %*% d) <= c2){
      break
    }
    
    if(f(theta.prime,...) >= f(theta,...) + alpha* gamma){
      alpha = alpha/2
    }
    
    if(g.prime %*% d /(g %*% d) > c2){
      alpha = 1.1 * alpha
    }
    theta.prime = theta + alpha*d
  }
  
  if(f(theta.prime,...) >= f(theta,...)){ ## if 
    alpha=1
    warning("we cannot find a valid step size")
  }
    
  print(alpha) ## prints steps alpha
  alpha
}

                                                              ## 3. Bfgs ##

## bfgs function returns a (named) list containing: (1) f -the scalar value of the objective function at the minimum, 
## (2) theta -the vector of values of the parameters at the minimum, (3) iter- the number of iterations taken to reach 
## the minimum, (4) g - the gradient vector at the minimum and (5) H - the approximate Hessian matrix. Bfgs is a function of
## 1.theta - a vector of initial values for the optimization parameters
## 2.f- is the objective function to minimize.  Its first argument is the vector of optimization parameters. Its second
## argument is a logical indicating whether or not gradients of the objective w.r.t. the parameters should be computed
## Remaining arguments are passed from bfgs using ‘...’.
## tol is the convergence tolerance
## fscale is a rough estimate of the magnitude of f at the optimum - used in convergence testing
## maxit is the maximum number of BFGS iterations to try before giving up
bfgs <- function(theta,f,...,tol=1e-5,fscale=1,maxit=100){
  theta.k = theta
  B.k <- I <- diag(length(theta))
  f0 = f(theta.k,...)
  eps = 1e-7 ## finite difference interval
  g.k = get_grad(f=f , ...,theta = theta.k,eps =  eps) ## initialize a random gradient
  
  if(any(is.infinite(g.k))) stop("gradient is not finite") ## 
  if(is.infinite(f0)) stop("objective value is not finite")
  
  iter = 0
  
  while(max(abs(g.k)) >= (abs(f0)+fscale)*tol && iter <= maxit){
    f0 = f(theta.k,...)
    print(f0)
    
    g.k = get_grad(f = f,..., theta = theta.k, eps =  eps)
    
    
    d = -B.k %*% g.k
    
    
    alpha = get_step_size(f = f,... ,theta = theta.k, d = d)

    theta.kprime = as.vector(theta.k + alpha * d)
    cat("thetaprime",theta.kprime,"\n")
    g.kprime = get_grad(f = f,..., theta = theta.kprime, eps = eps)
    cat("gkprime",g.kprime,"\n")
    
    
    s.k = theta.kprime - theta.k
    y.k = g.kprime - g.k
    
    rho.k.i = sum(s.k*y.k)
    rho.k = 1/rho.k.i
    
    
    ## faster implementation
    B.kprime = B.k + ((rho.k.i+ as.vector(t(y.k)%*%B.k%*%y.k))*s.k %*% t(s.k))/(rho.k.i**2) - (B.k%*%y.k%*%t(s.k)+s.k%*%(t(y.k)%*%B.k))/rho.k.i
    
    theta.k = theta.kprime
    B.k = B.kprime
    iter = iter + 1
    
  }
  
  if(iter > maxit) warning("max iteration has been reached")
  
  
  ## fast way to compute H as B^-1
  H = chol2inv(chol(B.k))
  
  if (!isSymmetric(H)){ ## if 
    H <- 0.5 * (t(H) + H)
  }
  
  list(f=f, theta=theta.k, iter=iter, g=g.k, H=H) # returns list of 
}